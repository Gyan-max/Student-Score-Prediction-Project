{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Score Prediction Analysis\n",
    "\n",
    "This notebook provides an interactive analysis of student performance prediction based on study habits.\n",
    "\n",
    "## Project Overview\n",
    "- **Objective**: Predict student final exam scores using study hours and attendance data\n",
    "- **Method**: Linear Regression\n",
    "- **Features**: Hours_Studied, Attendance\n",
    "- **Target**: Final_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data_processor import DataProcessor\n",
    "from visualizer import DataVisualizer\n",
    "from model_trainer import ModelTrainer\n",
    "from predictor import ScorePredictor\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "data_processor = DataProcessor()\n",
    "visualizer = DataVisualizer()\n",
    "\n",
    "# Load data\n",
    "data_path = '../data/student_data.csv'\n",
    "df = data_processor.load_data(data_path)\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst 5 records:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics\n",
    "visualizer.display_summary_stats(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots showing relationships\n",
    "visualizer.plot_scatter_relationships(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation heatmap\n",
    "visualizer.plot_correlation_heatmap(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data distributions\n",
    "visualizer.plot_distribution(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess data\n",
    "df_clean = data_processor.clean_data(df)\n",
    "\n",
    "print(f\"Original dataset: {len(df)} records\")\n",
    "print(f\"Cleaned dataset: {len(df_clean)} records\")\n",
    "print(f\"Records removed: {len(df) - len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = data_processor.split_data(df_clean)\n",
    "\n",
    "print(\"Data split completed:\")\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Testing set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the linear regression model\n",
    "trainer = ModelTrainer()\n",
    "model = trainer.train_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance\n",
    "metrics = trainer.evaluate_model(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Scatter plot of predictions vs actual\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(metrics['actual'], metrics['predictions'], alpha=0.6)\n",
    "plt.plot([0, 100], [0, 100], 'r--', alpha=0.8)\n",
    "plt.xlabel('Actual Scores')\n",
    "plt.ylabel('Predicted Scores')\n",
    "plt.title('Predictions vs Actual Scores')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals plot\n",
    "plt.subplot(1, 2, 2)\n",
    "residuals = metrics['actual'] - metrics['predictions']\n",
    "plt.scatter(metrics['predictions'], residuals, alpha=0.6)\n",
    "plt.axhline(y=0, color='r', linestyle='--', alpha=0.8)\n",
    "plt.xlabel('Predicted Scores')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals Plot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load model for prediction\n",
    "model_path = '../models/student_score_model.pkl'\n",
    "trainer.save_model(model_path)\n",
    "\n",
    "# Initialize predictor\n",
    "predictor = ScorePredictor()\n",
    "predictor.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prediction: 4 hours studied, 80% attendance\n",
    "hours = 4\n",
    "attendance = 80\n",
    "\n",
    "prediction = predictor.predict_score(hours, attendance)\n",
    "print(f\"Prediction for {hours} hours studied and {attendance}% attendance:\")\n",
    "print(f\"Expected Final Score: {prediction:.1f}/100\")\n",
    "\n",
    "# Get detailed explanation\n",
    "explanation = predictor.get_prediction_explanation(hours, attendance)\n",
    "print(\"\\n\" + explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different scenarios\n",
    "scenarios = [\n",
    "    (6, 80),    # More study hours\n",
    "    (4, 95),    # Better attendance\n",
    "    (6, 95),    # Both improved\n",
    "    (2, 60),    # Both reduced\n",
    "]\n",
    "\n",
    "comparison_df = predictor.compare_scenarios(hours, attendance, scenarios)\n",
    "print(\"Scenario Comparison:\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Prediction Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive prediction function\n",
    "def interactive_prediction():\n",
    "    \"\"\"Interactive tool for making predictions.\"\"\"\n",
    "    print(\"Student Score Prediction Tool\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    try:\n",
    "        hours = float(input(\"Enter hours studied per day (0-24): \"))\n",
    "        attendance = float(input(\"Enter attendance percentage (0-100): \"))\n",
    "        \n",
    "        prediction = predictor.predict_score(hours, attendance)\n",
    "        explanation = predictor.get_prediction_explanation(hours, attendance)\n",
    "        \n",
    "        print(f\"\\nPredicted Score: {prediction:.1f}/100\")\n",
    "        print(\"\\n\" + explanation)\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "# Uncomment the line below to run the interactive tool\n",
    "# interactive_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Insights and Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "1. **Strong Correlation**: Attendance shows stronger correlation with final scores than study hours\n",
    "2. **Model Performance**: The linear regression model provides reasonable predictions\n",
    "3. **Feature Importance**: Both study hours and attendance contribute significantly to final scores\n",
    "\n",
    "### Model Equation:\n",
    "The trained model follows the equation:\n",
    "```\n",
    "Final_Score = Intercept + (Hours_Coefficient × Hours_Studied) + (Attendance_Coefficient × Attendance)\n",
    "```\n",
    "\n",
    "### Recommendations:\n",
    "- Students should focus on both consistent attendance and adequate study time\n",
    "- Attendance appears to be slightly more important than study hours\n",
    "- The model can be used for early intervention to identify at-risk students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model summary\n",
    "coeffs = trainer.get_model_coefficients()\n",
    "print(\"Final Model Summary:\")\n",
    "print(\"=\" * 25)\n",
    "print(f\"R² Score: {metrics['r2_score']:.4f}\")\n",
    "print(f\"Mean Absolute Error: {metrics['mean_absolute_error']:.2f} points\")\n",
    "print(f\"Model Intercept: {coeffs['intercept']:.2f}\")\n",
    "print(f\"Hours Studied Coefficient: {coeffs['coefficients']['Hours_Studied']:.2f}\")\n",
    "print(f\"Attendance Coefficient: {coeffs['coefficients']['Attendance']:.2f}\")\n",
    "\n",
    "print(\"\\nModel saved to:\", model_path)\n",
    "print(\"\\nProject completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}